{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "278a317c-743c-4853-93f3-636ea1bb1245",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Users/thanakrit.boonquarmdee@lotuss.com/utils/std_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3f3742e-74fc-4377-a405-d294555abc3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from edm_class import txnItem\n",
    "\n",
    "from scipy.stats import expon\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import expon\n",
    "\n",
    "spark.sparkContext.setCheckpointDir('dbfs:/FileStore/thanakrit/temp/checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db61b2b9-cfec-4df8-9350-bde7e6d00f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prep data\n",
    "----\n",
    "- Snap transaction\n",
    "- Join to filter only needed product list\n",
    "- Aggregate unit to hh_id , date_id level\n",
    "- Find previous_shp_date, number of day difference form previous date\n",
    "- Checkpoint for faster processing\n",
    "\"\"\"\n",
    "txn_spec = txnItem(end_wk_id=202248, range_n_week=1, str_wk_id=202245, customer_data='CC')\n",
    "all_txn = txn_spec.get_txn()\n",
    "\n",
    "focus_prod = spark.createDataFrame([(88196,)],[\"upc_id\",])\n",
    "prd_txn = all_txn.join(focus_prod, \"upc_id\", \"inner\")\n",
    "\n",
    "hh_shp_date = \\\n",
    "(prd_txn\n",
    " .groupby(\"household_id\", \"date_id\")\n",
    " .agg(F.sum(\"pkg_weight_unit\").alias(\"units\"))\n",
    ")\n",
    "\n",
    "hh_shp_pre = (hh_shp_date\n",
    "              .withColumn(\"prev_date_id\", F.lag(F.col(\"date_id\")).over(Window.partitionBy(\"household_id\").orderBy(\"date_id\")))\n",
    "              .withColumn(\"day_diff\", F.datediff(end=\"date_id\", start=\"prev_date_id\"))\n",
    "             )\n",
    "\n",
    "hh_shp_date_diff = hh_shp_pre.where(F.col(\"day_diff\").isNotNull())\n",
    "\n",
    "hh_shp_date_diff = hh_shp_date_diff.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "444e689b-62e0-4f1e-9c1d-609eaa4cd0ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hh_shp_date_diff.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d00186d4-97b5-42eb-a970-8ad09424e38c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Estimated next purchase cycle with Exponential Smoothing\n",
    "----\n",
    "Backtest for best smoothing parameter (alpha)\n",
    "\n",
    "\"\"\"\n",
    "def backtest_smooth_purchase_cyc_alpha(sf, test_alpha):\n",
    "    \"\"\"Estimate next purchase cycle, with exponential smoothing + back-testing to find optimum smoothing factor (test_alpha).\n",
    "    In back-testing process, use latest(T=1) purchase cycle as target, use T=2, T=3 of purchase cycle as features.\n",
    "    \n",
    "    If purchase cycle at T=3 not available, then ignore smoothing and use T=2 as prediction of target.\n",
    "    \n",
    "    Exponential smoothing formula\n",
    "    yt = alpha*y,t-1 + (1-alpha)*y,t-2\n",
    "    \"\"\"\n",
    "    \n",
    "    alpha = test_alpha\n",
    "    alpha_1 = (1-test_alpha)\n",
    "    MAX_LATEST_NUM_PURCHASE = 3\n",
    "\n",
    "    n = sf.count()\n",
    "    out = (sf\n",
    "           .withColumn(\"desc_order\", F.row_number().over(Window.partitionBy(\"household_id\").orderBy(F.col(\"date_id\").desc_nulls_last())))\n",
    "           .where(F.col(\"desc_order\")<=MAX_LATEST_NUM_PURCHASE)\n",
    "           .groupBy(\"household_id\")\n",
    "           .pivot(\"desc_order\")\n",
    "           .agg(F.sum(\"day_diff\"))  # F.sum no effect , just pivot\n",
    "           .withColumn(\"smth_prchs_cyc\", F.when(F.col(\"3\").isNotNull(), F.col(\"2\")*alpha + F.col(\"3\")*alpha_1).otherwise(F.col(\"2\")))\n",
    "           .where(F.col(\"smth_prchs_cyc\").isNotNull())\n",
    "           .withColumn(\"abs_error\", F.abs(F.col(\"smth_prchs_cyc\")-F.col(\"1\")))\n",
    "    )\n",
    "    mean_abs_err = out.agg(F.mean(\"abs_error\")).collect()[0][0]\n",
    "    #   print(f\"Test alpha {test_alpha} -> mean_abs_error (obs {n}) : {abs_err/n:.6f}\")\n",
    "    return out, mean_abs_err\n",
    "  \n",
    "def find_min_alpha(df):\n",
    "    \"\"\"Find smooth factor that minimize mean abs error\n",
    "    Hyperparam  from 0.3 - 0.75 with step 0.01\n",
    "    \"\"\"\n",
    "    \n",
    "    from matplotlib import pyplot as plt\n",
    "    alpha = []\n",
    "    mae = []\n",
    "    for a in np.arange(0.3, 0.75, 0.01):\n",
    "        alpha.append(a)\n",
    "        _, err = backtest_smooth_purchase_cyc_alpha(df, test_alpha=a)\n",
    "        mae.append(err)\n",
    "\n",
    "    err_df = pd.DataFrame({\"alpha\":alpha, \"mae\":mae})\n",
    "    fig, ax = plt.subplots()\n",
    "    err_df.plot(x=\"alpha\", y=\"mae\", ax=ax)\n",
    "    plt.title(\"Mean Absolute Error\")\n",
    "    plt.show()\n",
    "\n",
    "    min_alpha = float(err_df.loc[err_df[\"mae\"].idxmin(), \"alpha\"])\n",
    "\n",
    "    return min_alpha\n",
    "\n",
    "def get_smooth_purchase_cyc(sf):\n",
    "    \"\"\"Get smoothed purchased cycle, based on exponential smoothing\n",
    "    \n",
    "    Exponential smoothing formula\n",
    "    yt = alpha*y,t-1 + (1-alpha)*y,t-2\n",
    "    \"\"\"\n",
    "    alpha = find_min_alpha(sf)\n",
    "    print(\"Exponential smoothing with smooth factor : {alpha:f.4}\")\n",
    "    alpha_1 = 1-alpha\n",
    "    MAX_LATEST_NUM_PURCHASE = 2\n",
    "    \n",
    "    out = (sf\n",
    "           .withColumn(\"desc_order\", F.row_number().over(Window.partitionBy(\"household_id\").orderBy(F.col(\"date_id\").desc_nulls_last())))\n",
    "           .where(F.col(\"desc_order\")<=MAX_LATEST_NUM_PURCHASE)\n",
    "           .groupBy(\"household_id\")\n",
    "           .pivot(\"desc_order\")\n",
    "           .agg(F.sum(\"day_diff\"))\n",
    "           .withColumn(\"smth_prchs_cyc\", F.when(F.col(\"2\").isNotNull(), F.col(\"1\")*alpha + F.col(\"2\")*alpha_1).otherwise(F.col(\"1\")))\n",
    "           .select(\"household_id\", \"smth_prchs_cyc\")\n",
    "    )\n",
    "\n",
    "    return out  \n",
    "\n",
    "@udf(\"float\")\n",
    "def get_prob_next_purchase(purchase_cycle_day: float,\n",
    "                           day_last_to_cmp_str: float,\n",
    "                           day_last_to_cmp_end: float):\n",
    "    \"\"\"Probability of next purchase fall into campaign period\n",
    "    Assume the next purchase event follow Exponentail distribution\n",
    "    \"\"\"\n",
    "    from scipy.stats import expon\n",
    "    \n",
    "    prob_til_cmp_str = expon(scale=purchase_cycle_day).cdf(x=day_last_to_cmp_str)\n",
    "    prob_til_cmp_end = expon(scale=purchase_cycle_day).cdf(x=day_last_to_cmp_end)\n",
    "    prob_in_cmp = prob_til_cmp_end - prob_til_cmp_str\n",
    "\n",
    "    return float(prob_in_cmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "970ee5a7-7f0a-423a-8c15-160bc67d28a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main\n",
    "\"\"\"\n",
    "def get_prop_purchase_in_cmp_period(hh_shp_date_diff: SparkDataFrame, \n",
    "                                    cmp_str_date: str,\n",
    "                                    cmp_period: int):\n",
    "    \"\"\"Extended probability of next N purchases fall into campaign period\n",
    "    For short purchase cycle, then then next purchase still not reach campaign start date\n",
    "    Assume that customer keep buying until purchase fall into campaing period\n",
    "    Assume each purchase cycle are independent, then overall propability = P(event_1)*P(event_2)*P(event_3)*...\n",
    "    \n",
    "    Param\n",
    "    ----\n",
    "    hh_shp_date_diff\n",
    "        Dataframe for calculate next purchase, run from step 1\n",
    "    cmp_str_date\n",
    "        campaign start date, \"YYYY-MM-DD\"\n",
    "    cmp_period\n",
    "        Number of campaign period id days\n",
    "    \"\"\"\n",
    "\n",
    "    cmp_str_date_id = datetime.strptime(cmp_str_date, \"%Y-%m-%d\")\n",
    "    cmp_period_day = cmp_period\n",
    "\n",
    "    last_purchase_cyc = \\\n",
    "    (hh_shp_date_diff\n",
    "     .groupBy(\"household_id\")\n",
    "     .agg(F.max(\"date_id\").alias(\"last_purchase_date_id\"))\n",
    "     .join(hh_smth_prchs_cyc, \"household_id\", \"outer\")\n",
    "    )\n",
    "\n",
    "    nxt_purchase = \\\n",
    "    (last_purchase_cyc\n",
    "     .withColumn(\"cmp_str_date_id\", F.lit(cmp_str_date_id).cast(\"date\"))\n",
    "     .withColumn(\"day_last_prchs_to_cmp_str\", F.datediff(F.col(\"cmp_str_date_id\") , F.col(\"last_purchase_date_id\")))\n",
    "     # number of purchase until next purchase fall into campaign\n",
    "     .withColumn(\"num_rolling_purchase_before_cmp_str\", F.expr(\" int(floor(day_last_prchs_to_cmp_str / smth_prchs_cyc)) \"))\n",
    "     # prop of each rolling purchase\n",
    "     .withColumn(\"prop_rolling_purchase\", get_prob_next_purchase(F.col(\"smth_prchs_cyc\"), F.lit(0.0), F.col(\"smth_prchs_cyc\")))\n",
    "     # last rolling purchase that the next will be in the campaign period\n",
    "     .withColumn(\"rolling_purchase_before_cmp_str_date\", F.expr(\" date_add(last_purchase_date_id, int(floor(day_last_prchs_to_cmp_str / smth_prchs_cyc) * smth_prchs_cyc)) \"))\n",
    "     # Calculate \n",
    "     .withColumn(\"day_til_cmp_str\", F.datediff(F.lit(cmp_str_date_id) , F.col(\"rolling_purchase_before_cmp_str_date\")))\n",
    "     .withColumn(\"day_til_cmp_end\", F.col(\"day_til_cmp_str\")+cmp_period_day)\n",
    "     # all prop of N rolling purchase\n",
    "     .withColumn(\"prop_prior_cmp\", F.when(F.col(\"num_rolling_purchase_before_cmp_str\")==0, F.lit(1.0) ).otherwise(F.pow(F.col(\"prop_rolling_purchase\"), F.col(\"num_rolling_purchase_before_cmp_str\"))))\n",
    "     # last prob that purchase fall into campaign <- Optimistic propbability not take rolling shoping into account\n",
    "     .withColumn(\"prop_pre_cmp\", get_prob_next_purchase(F.col(\"smth_prchs_cyc\"), F.col(\"day_til_cmp_str\"), F.col(\"day_til_cmp_end\")))\n",
    "     # Final prop = all rolling prop x last prop <- Presimistic propbability take \n",
    "     .withColumn(\"prop\", F.col(\"prop_prior_cmp\")*F.col(\"prop_pre_cmp\"))\n",
    "    #  .withColumn(\"day_til_cmp_end_2\", F.col(\"day_last_prchs_to_cmp_str\")+cmp_period_day)\n",
    "    #  .withColumn(\"prop_2\", get_prob_next_purchase(F.col(\"smth_prchs_cyc\"), F.col(\"day_last_prchs_to_cmp_str\"), F.col(\"day_til_cmp_end_2\")))\n",
    "     .select(\"household_id\", \"last_purchase_date_id\", F.col(\"smth_prchs_cyc\").alias(\"predicted_purchase_cycle\"), \"cmp_str_date_id\", \"prop\")\n",
    "    )\n",
    "    \n",
    "    return nxt_purchase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71a53b5b-2ac5-4f1f-abb7-35be0b5fb8f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_prop_purchase_in_cmp_period(hh_shp_date_diff, \"2022-12-31\", 14).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "prob_next_purchase",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
